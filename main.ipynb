{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Unlearning: Korean Legal Knowledge Editing\n",
    "\n",
    "**Modification and Deletion of Knowledge in Korean Legal Domain | KT-Korea University Joint Research**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a conda environment\n",
    "\n",
    "```bash\n",
    "conda create -n unlearning python==3.10.0 -y\n",
    "conda activate unlearning\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Make sure to have an `.env` file in the project root directory containing your OpenAI API key:\n",
    "\n",
    "```plaintext\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/raid6/junkim100/miniconda3/envs/kt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import openai\n",
    "from openai.error import APIError\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir: str = \"./법령지식\"\n",
    "output_dir: str = \"./results\"\n",
    "final_dir: str = \"./final_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize model with automatic device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Get the device of the first model parameter for input tensors\n",
    "model_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(input_file: str) -> dict:\n",
    "    \"\"\"Extracts label and full_text pairs from JSON files in the specified directory.\"\"\"\n",
    "    result = {}\n",
    "    if input_file.endswith(\".json\"):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            count = 0\n",
    "            for i, key in enumerate(data):\n",
    "                try:\n",
    "                    # Skip entries with 'comment'\n",
    "                    if 'comment' in data[key]:\n",
    "                        continue\n",
    "                        \n",
    "                    label = data[key][\"label\"]\n",
    "                    full_text = data[key][\"fullText\"]\n",
    "                    if len(full_text) < 3 * len(label) and len(full_text) < 50:\n",
    "                        continue\n",
    "                    result[label] = full_text\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            print(f\"{count} out of {len(data)} were successfully extracted from {input_file}\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input directory.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main two types of queries we can ask are\n",
    "\n",
    "1. Give the law name and ask for explanation\n",
    "2. Give the explanation and ask for law name\n",
    "\n",
    "For each type, we can try giving system prompts in different languages and different complexity:\n",
    "\n",
    "- Simple Korean\n",
    "- Detailed Korean\n",
    "- Simple English\n",
    "- Detailed English\n",
    "\n",
    "Notably, the English prompts include the phrase: \"You must respond in Korean.\"\n",
    "\n",
    "This will result in 8 total system prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of system prompts\n",
    "system_prompts = {\n",
    "    \"type1\": {\n",
    "        \"simple\": {\n",
    "            \"korean\": [\"다음 법령의 조항을 말해주세요.\"],\n",
    "            \"english\": [\"Please state the provisions of the following law. You must respond in Korean.\"]\n",
    "        },\n",
    "        \"detailed\": {\n",
    "            \"korean\": [\"다음은 대한민국의 법령입니다. 법령의 조항을 말해주세요.\"],\n",
    "            \"english\": [\"The following is a law of the Republic of Korea. Please state the provisions of the law. You must respond in Korean.\"]\n",
    "        }\n",
    "    },\n",
    "    \"type2\": {\n",
    "        \"simple\": {\n",
    "            \"korean\": [\"다음 법령 조항을 읽고 법률의 이름을 알려주세요.\"],\n",
    "            \"english\": [\"Please read the following law provision and tell me the name of the law. You must respond in Korean.\"]\n",
    "        },\n",
    "        \"detailed\": {\n",
    "            \"korean\": [\"다음은 대한민국의 법령입니다. 법령 조항을 읽고 법률의 이름을 알려주세요.\"],\n",
    "            \"english\": [\"The following is a law of the Republic of Korea. Please read the law provision and tell me the name of the law. You must respond in Korean.\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also give a single shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot = {\n",
    "    \"name\": \"119긴급신고법 제 18조의 제1항\",\n",
    "    \"provision\": \"① 소방청장은 「전파법」 제9조제1항제1호에 따라 소방업무용으로 할당된 무선통신 주파수를 효율적으로 운영하여야 한다. ② 제1항에 따른 소방업무용 주파수의 운영에 필요한 사항은 행정안전부령으로 정한다.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a function for creating messages with different system prompts and types for the same law/provision pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(system_prompt: dict, shot: dict, label: str, full_text: str) -> dict:\n",
    "    messages_dict = {}\n",
    "    \n",
    "    create_type1 = lambda x: [\n",
    "        {\"role\": \"system\", \"content\": x[0]},\n",
    "        {\"role\": \"user\", \"content\": shot[\"name\"]},\n",
    "        {\"role\": \"assistant\", \"content\": shot[\"provision\"]},\n",
    "        {\"role\": \"user\", \"content\": label}\n",
    "    ]\n",
    "    \n",
    "    create_type2 = lambda x: [\n",
    "        {\"role\": \"system\", \"content\": x[0]},\n",
    "        {\"role\": \"user\", \"content\": shot[\"provision\"]},\n",
    "        {\"role\": \"assistant\", \"content\": shot[\"name\"]},\n",
    "        {\"role\": \"user\", \"content\": full_text}\n",
    "    ]\n",
    "    \n",
    "    # Create message variations\n",
    "    for type_key in system_prompt:\n",
    "        messages_dict[type_key] = {}\n",
    "        for complexity in system_prompt[type_key]:\n",
    "            messages_dict[type_key][complexity] = {}\n",
    "            for lang in system_prompt[type_key][complexity]:\n",
    "                messages_dict[type_key][complexity][lang] = {}\n",
    "                creator = create_type1 if type_key == \"type1\" else create_type2\n",
    "                messages_dict[type_key][complexity][lang] = creator(\n",
    "                    system_prompt[type_key][complexity][lang]\n",
    "                )\n",
    "    \n",
    "    return messages_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test create_message() with a sample law/provision pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"자동차손해배상 보장법 제45조의2 제1항\"\n",
    "sample_provision = \"제45조의2 (정보의 제공 및 관리)  ① 제45조제3항에 따라 업무를 위탁받은 보험요율산출기관은 같은 조 제1항에 따라 업무를 위탁받은 자의 요청이 있는 경우 제공할 정보의 내용 등 대통령령으로 정하는 범위에서 가입관리전산망에서 관리되는 정보를 제공할 수 있다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_messages_dict = create_messages(system_prompts, shot, sample_name, sample_provision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Please state the provisions of the following law. You must respond in Korean.'},\n",
       " {'role': 'user', 'content': '119긴급신고법 제 18조의 제1항'},\n",
       " {'role': 'assistant',\n",
       "  'content': '① 소방청장은 「전파법」 제9조제1항제1호에 따라 소방업무용으로 할당된 무선통신 주파수를 효율적으로 운영하여야 한다. ② 제1항에 따른 소방업무용 주파수의 운영에 필요한 사항은 행정안전부령으로 정한다.'},\n",
       " {'role': 'user', 'content': '자동차손해배상 보장법 제45조의2 제1항'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_messages_dict[\"type1\"][\"simple\"][\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': '다음은 대한민국의 법령입니다. 법령 조항을 읽고 법률의 이름을 알려주세요.'},\n",
       " {'role': 'user',\n",
       "  'content': '① 소방청장은 「전파법」 제9조제1항제1호에 따라 소방업무용으로 할당된 무선통신 주파수를 효율적으로 운영하여야 한다. ② 제1항에 따른 소방업무용 주파수의 운영에 필요한 사항은 행정안전부령으로 정한다.'},\n",
       " {'role': 'assistant', 'content': '119긴급신고법 제 18조의 제1항'},\n",
       " {'role': 'user',\n",
       "  'content': '제45조의2 (정보의 제공 및 관리)  ① 제45조제3항에 따라 업무를 위탁받은 보험요율산출기관은 같은 조 제1항에 따라 업무를 위탁받은 자의 요청이 있는 경우 제공할 정보의 내용 등 대통령령으로 정하는 범위에서 가입관리전산망에서 관리되는 정보를 제공할 수 있다.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_messages_dict[\"type2\"][\"detailed\"][\"korean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(messages):\n",
    "    \"\"\"Generate a response using the model.\"\"\"\n",
    "\n",
    "    def format_prompt(messages):\n",
    "        \"\"\"Format messages into a single prompt string.\"\"\"\n",
    "        prompt = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                prompt += f\"Instructions: {message['content']}\\n\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                prompt += f\"Input: {message['content']}\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                prompt += f\"Output: {message['content']}\\n\\n\"\n",
    "        prompt += \"Output:\"  # Add this to indicate where the model should generate\n",
    "        return prompt\n",
    "\n",
    "    # Format the input messages into a single string\n",
    "    prompt = format_prompt(messages)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model_device)\n",
    "\n",
    "    # Generate output with refined parameters\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract only the generated response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the input part from the response\n",
    "    generated_response = full_response[len(prompt):].strip()\n",
    "    \n",
    "    if '\\n\\nInput:' in generated_response:\n",
    "        generated_response = generated_response.split('\\n\\nInput:')[0]\n",
    "    \n",
    "    return prompt, generated_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating the response for the first items of the 층간소음법령.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = extract_data(f\"{input_dir}/층간소음법령.json\")\n",
    "\n",
    "for label, full_text in list(sample_data.items())[:1]:\n",
    "    messages_dict = create_messages(system_prompts, shot, label, full_text)\n",
    "    \n",
    "    for type_key in messages_dict:\n",
    "        for complexity_key in messages_dict[type_key]:\n",
    "            for lang_key in messages_dict[type_key][complexity_key]:\n",
    "                messages = messages_dict[type_key][complexity_key][lang_key]\n",
    "                \n",
    "                print(f\"\\nType: {type_key}, Complexity: {complexity_key}, Language: {lang_key}\")\n",
    "                response = generate_response(messages)\n",
    "                # print(f\"messages: {messages}\")\n",
    "                print(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try using the BLEU-4, ROUGE-1, ROUGE-L score, and also use GPT-4o to evaluate the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    chencherry = SmoothingFunction()\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "\n",
    "    bleu1 = sentence_bleu([ref_tokens], hyp_tokens, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n",
    "    bleu4 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1)\n",
    "    return bleu1, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(reference, hypothesis):\n",
    "    \"\"\"Calculates ROUGE-1 (unigram overlap) and ROUGE-L (longest common subsequence).\"\"\"\n",
    "\n",
    "    def lcs_length(ref_tokens, hyp_tokens):\n",
    "        \"\"\"Helper function to calculate the length of the longest common subsequence (LCS).\"\"\"\n",
    "        ref_len = len(ref_tokens)\n",
    "        hyp_len = len(hyp_tokens)\n",
    "        \n",
    "        # Create a 2D table to store lengths of longest common subsequence\n",
    "        lcs_table = [[0] * (hyp_len + 1) for _ in range(ref_len + 1)]\n",
    "        \n",
    "        for i in range(1, ref_len + 1):\n",
    "            for j in range(1, hyp_len + 1):\n",
    "                if ref_tokens[i - 1] == hyp_tokens[j - 1]:\n",
    "                    lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
    "        \n",
    "        return lcs_table[ref_len][hyp_len]\n",
    "    \n",
    "    # Tokenize the reference and hypothesis\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "\n",
    "    # --- ROUGE-1 ---\n",
    "    # Calculate precision and recall for unigrams\n",
    "    precision_1 = len(set(ref_tokens) & set(hyp_tokens)) / len(hyp_tokens) if len(hyp_tokens) > 0 else 0\n",
    "    recall_1 = len(set(ref_tokens) & set(hyp_tokens)) / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score for ROUGE-1\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0\n",
    "\n",
    "    # --- ROUGE-L ---\n",
    "    # Calculate LCS length\n",
    "    lcs_len = lcs_length(ref_tokens, hyp_tokens)\n",
    "\n",
    "    # Precision, Recall for LCS\n",
    "    precision_l = lcs_len / len(hyp_tokens) if len(hyp_tokens) > 0 else 0\n",
    "    recall_l = lcs_len / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score for ROUGE-L\n",
    "    f1_l = 2 * (precision_l * recall_l) / (precision_l + recall_l) if (precision_l + recall_l) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"rouge_1\": {\"precision\": precision_1, \"recall\": recall_1, \"f1\": f1_1},\n",
    "        \"rouge_l\": {\"precision\": precision_l, \"recall\": recall_l, \"f1\": f1_l}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the GPT-4o with openai api and instruct it to score either 0, 1, or 2 where 2 is the highest score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def calculate_gpt(response: str, reference: str, retries=3, delay=5) -> int:\n",
    "    \"\"\"Scores a response based on its similarity to a reference using GPT with retry logic.\"\"\"\n",
    "    prompt = (\n",
    "        f\"Score the following response based on its similarity to the reference. \"\n",
    "        f\"The score should be either 0, 1, or 2 where 2 is the highest score.\\n\\n\"\n",
    "        f\"Reference: {reference}\\n\\n\"\n",
    "        f\"Response: {response}\\n\\n\"\n",
    "        f\"Provide only a single number as the score.\"\n",
    "    )\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Call OpenAI API with refined prompt\n",
    "            api_response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an evaluator that scores responses based on their similarity to a reference.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1,\n",
    "                temperature=0,\n",
    "            )\n",
    "\n",
    "            # Extract and validate the score from the API response\n",
    "            score_text = api_response['choices'][0]['message']['content'].strip()\n",
    "            score = int(score_text)  # Try converting to an integer\n",
    "            if score in [0, 1, 2]:   # Ensure it's within expected range\n",
    "                return score\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid score received: {score_text}\")\n",
    "\n",
    "        except APIError as e:\n",
    "            if e.http_status == 500:\n",
    "                print(f\"Server error (500), retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"API Error: {e}\")\n",
    "                break\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Error parsing GPT score: {e}\")\n",
    "            return -1  # Return a default or error value in case of failure\n",
    "\n",
    "    print(\"Max retries reached. Returning default score -1.\")\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try getting the scores for the first item of the sample_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = extract_data(f\"{input_dir}/층간소음법령.json\")\n",
    "\n",
    "for label, full_text in list(sample_data.items())[:1]:\n",
    "    messages_dict = create_messages(system_prompts, shot, label, full_text)\n",
    "    \n",
    "    for type_key in messages_dict:\n",
    "        for complexity_key in messages_dict[type_key]:\n",
    "            for lang_key in messages_dict[type_key][complexity_key]:\n",
    "                messages = messages_dict[type_key][complexity_key][lang_key]\n",
    "                \n",
    "                print(f\"\\nType: {type_key}, Complexity: {complexity_key}, Language: {lang_key}\")\n",
    "                prompt, response = generate_response(messages)\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                print(f\"Generated Response: {response}\")\n",
    "\n",
    "                # Calculate BLEU score\n",
    "                bleu_score = calculate_bleu(label, response)\n",
    "                print(f\"BLEU-1 score: {bleu_score[0]:.4f}\")\n",
    "                print(f\"BLEU-4 score: {bleu_score[1]:.4f}\")\n",
    "\n",
    "                # Calculate ROUGE scores\n",
    "                rouge_scores = calculate_rouge(label, response)\n",
    "                print(f\"ROUGE-1: {rouge_scores['rouge_1']['f1']:.4f}\")\n",
    "                print(f\"ROUGE-L: {rouge_scores['rouge_l']['f1']:.4f}\")\n",
    "\n",
    "                # Score the response\n",
    "                score = calculate_gpt(response, label)\n",
    "                print(f\"GPT-4o Score: {score}\")\n",
    "                print(\"////////////////////////////////////////////////////////////////////////////////////////\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we saw how it works, let's make this into a function that saves it into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_response(dataset: dict, output_file: str):\n",
    "    \"\"\"Generates responses for a dataset and saves them to a JSON file.\"\"\"\n",
    "    responses = []\n",
    "\n",
    "    # Initialize progress bar\n",
    "    bar = tqdm.tqdm(total=len(dataset)*8, desc=\"Generating Responses\", unit=\"entry\")\n",
    "\n",
    "    for id, (label, full_text) in enumerate(dataset.items()):\n",
    "        messages_dict = create_messages(system_prompts, shot, label, full_text)\n",
    "        \n",
    "        for type_key in messages_dict:\n",
    "            for complexity_key in messages_dict[type_key]:\n",
    "                for lang_key in messages_dict[type_key][complexity_key]:\n",
    "                    # Get the message list directly\n",
    "                    message_list = messages_dict[type_key][complexity_key][lang_key]\n",
    "                    \n",
    "                    # Generate response using the improved method\n",
    "                    prompt, response = generate_response(message_list)\n",
    "                    \n",
    "                    # Calculate BLEU and ROUGE scores\n",
    "                    bleu_score = calculate_bleu(label, response)\n",
    "                    rouge_scores = calculate_rouge(label, response)\n",
    "                    \n",
    "                    # Use GPT-based scoring with error handling\n",
    "                    gpt_score = calculate_gpt(response, label)\n",
    "\n",
    "                    responses.append({\n",
    "                        \"id\": id,\n",
    "                        \"label\": label,\n",
    "                        \"full_text\": full_text,\n",
    "                        \"type\": type_key,\n",
    "                        \"complexity\": complexity_key,\n",
    "                        \"language\": lang_key,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"response\": response,\n",
    "                        \"bleu_1\": bleu_score[0],\n",
    "                        \"bleu_4\": bleu_score[1],\n",
    "                        \"rouge_1\": rouge_scores[\"rouge_1\"][\"f1\"],\n",
    "                        \"rouge_l\": rouge_scores[\"rouge_l\"][\"f1\"],\n",
    "                        \"gpt_score\": gpt_score\n",
    "                    })\n",
    "\n",
    "                    # Save responses to JSON file every time a response is generated to prevent data loss\n",
    "                    with open(output_file, \"w\") as f:\n",
    "                        json.dump(responses, f, indent=2, ensure_ascii=False)\n",
    "                    # Update progress bar\n",
    "                    bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a while...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        data = extract_data(os.path.join(input_dir, file))\n",
    "        output_file = os.path.join(output_dir, file)\n",
    "        eval_response(data, output_file)\n",
    "        print(f\"Responses saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then average the scores for the entries with the same id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_scores(responses: list, output_file: str):\n",
    "    \"\"\"Combines scores for entries with the same label and calculates average scores.\"\"\"\n",
    "    # Initialize list to store results\n",
    "    result = []\n",
    "    temp_scores = {}\n",
    "    \n",
    "    # Group responses by label\n",
    "    for response in responses:\n",
    "        try:\n",
    "            label = response[\"label\"]\n",
    "            if label not in temp_scores:\n",
    "                temp_scores[label] = {\n",
    "                    \"count\": 0,\n",
    "                    \"label\": label,\n",
    "                    \"full_text\": response[\"full_text\"],\n",
    "                    \"avg_bleu_1\": 0,\n",
    "                    \"avg_bleu_4\": 0,\n",
    "                    \"avg_rouge_1\": 0,\n",
    "                    \"avg_rouge_l\": 0,\n",
    "                    \"avg_gpt_score\": 0\n",
    "                }\n",
    "            \n",
    "            # Add scores\n",
    "            temp_scores[label][\"count\"] += 1\n",
    "            temp_scores[label][\"avg_bleu_1\"] += response[\"bleu_1\"]\n",
    "            temp_scores[label][\"avg_bleu_4\"] += response[\"bleu_4\"]\n",
    "            temp_scores[label][\"avg_rouge_1\"] += response[\"rouge_1\"]\n",
    "            temp_scores[label][\"avg_rouge_l\"] += response[\"rouge_l\"]\n",
    "            temp_scores[label][\"avg_gpt_score\"] += response[\"gpt_score\"]\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in response: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages and format output\n",
    "    for scores in temp_scores.values():\n",
    "        count = scores[\"count\"]\n",
    "        if count > 0:\n",
    "            entry = {\n",
    "                \"label\": scores[\"label\"],\n",
    "                \"full_text\": scores[\"full_text\"],\n",
    "                \"avg_bleu_1\": scores[\"avg_bleu_1\"] / count,\n",
    "                \"avg_bleu_4\": scores[\"avg_bleu_4\"] / count,\n",
    "                \"avg_rouge_1\": scores[\"avg_rouge_1\"] / count,\n",
    "                \"avg_rouge_l\": scores[\"avg_rouge_l\"] / count,\n",
    "                \"avg_gpt_score\": scores[\"avg_gpt_score\"] / count\n",
    "            }\n",
    "            result.append(entry)\n",
    "    \n",
    "    # Save as JSON array\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        input_path = os.path.join(output_dir, file)\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            responses = json.load(f)\n",
    "        output_file = os.path.join(output_dir, f\"avg_{file}\")\n",
    "        avg_scores(responses, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ranking Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the avg JSON files into one and sort them by the score of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sort_scores(input_file, output_file, metric):\n",
    "    \"\"\"\n",
    "    Combines multiple JSON files from input directory, sorts them by metric, and saves to output file\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON file\n",
    "        output_file (str): Path to save the sorted combined JSON\n",
    "        metric (str): Metric to sort by (e.g. 'avg_bleu_4')\n",
    "    \"\"\"\n",
    "    # List to store combined data from all files\n",
    "    combined_data = []\n",
    "\n",
    "    # Read each JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # Handle both single dict and list of dicts\n",
    "            if isinstance(data, dict):\n",
    "                combined_data.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                combined_data.extend(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error reading {input_file} - invalid JSON\")\n",
    "    \n",
    "    # Sort combined data based on metric\n",
    "    sorted_data = sorted(combined_data, key=lambda x: x[metric])\n",
    "    \n",
    "    # Save sorted data to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return sorted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the files using all metrics!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"bleu_1\", \"bleu_4\", \"rouge_1\", \"rouge_l\", \"gpt_score\"]\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.startswith(\"avg_\") and file.endswith(\".json\"):\n",
    "        input_file = os.path.join(output_dir, file)\n",
    "        for metric in metrics:\n",
    "            output_file = os.path.join(final_dir, f\"avg_{metric}.json\")\n",
    "            merge_and_sort_scores(input_file, output_file, metric=f\"avg_{metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top five entries for each metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five entries by bleu_1:\n",
      "\n",
      "{'label': '도로교통법 시행규칙 제19조 제3항', 'full_text': '③ 경찰청장 또는 지방경찰청장이 법 제17조제2항에 따라 구역 또는 구간을 지정하여 자동차등의 속도를 제한하려는 경우에는 「도로의 구조ㆍ시설기준에 관한 규칙」 제8조에 따른 설계속도, 실제 주행속도, 교통사고 발생 위험성, 도로주변 여건 등을 고려하여야 한다.<신설 2010.7.9.>', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '자동차손해배상 보장법 시행령 제12조의3 제2항', 'full_text': '② 보험회사등이 법 제14조제4항 전단에 따라 교통사고 관련 조사기록의 열람을 청구하는 경우에는 열람예정일 7일 전까지 열람청구서에 열람사유서를 첨부하여 경찰관서에 제출하여야 한다. 다만, 긴급하거나 부득이한 사유가 있음을 소명하는 경우에는 그러하지 아니하다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 시행규칙 제37조의3 제2항', 'full_text': '② 제1항에 따라 정보의 제공을 요청받은 경찰서장은 법 제53조의4에 따라 어린이 교육시설을 감독하는 주무기관의 장이 요청한 정보를 해당 주무기관의 장에게 제공할 수 있다. [본조신설 2014.12.31.]', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제74조 제2항', 'full_text': '② 지방경찰청장은 교통안전교육을 하기 위하여 다음 각 호의 어느 하나에 해당하는 기관이나 시설이 대통령령으로 정하는 시설ㆍ설비 및 강사 등의 요건을 갖추어 신청하는 경우에는 해당 기관이나 시설을 교통안전교육을 하는 기관(이하 \"\"교통안전교육기관\"\"이라 한다)으로 지정할 수 있다. 1. 제99조에 따른 자동차운전학원 2. 제120조 및 제121조에 따른 도로교통공단과 그 지부(支部)ㆍ지소 및 교육기관 3. 「평생교육법」 제30조제2항에 따른 평생교육과정이 개설된 대학 부설 평생교육시설 4. 제주특별자치도 또는 시ㆍ군ㆍ자치구에서 운영하는 교육시설', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '교통사고조사규칙 제15조 제4항', 'full_text': '④ 사진촬영을 할 때에는 목적물의 방향과 남은 흔적 등에 주의하고, 반드시 그 크기를 파악할 수 있도록 하여야 한다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "\n",
      "\n",
      "Top five entries by bleu_4:\n",
      "\n",
      "{'label': '도로교통법 시행규칙 제19조 제3항', 'full_text': '③ 경찰청장 또는 지방경찰청장이 법 제17조제2항에 따라 구역 또는 구간을 지정하여 자동차등의 속도를 제한하려는 경우에는 「도로의 구조ㆍ시설기준에 관한 규칙」 제8조에 따른 설계속도, 실제 주행속도, 교통사고 발생 위험성, 도로주변 여건 등을 고려하여야 한다.<신설 2010.7.9.>', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '자동차손해배상 보장법 시행령 제12조의3 제2항', 'full_text': '② 보험회사등이 법 제14조제4항 전단에 따라 교통사고 관련 조사기록의 열람을 청구하는 경우에는 열람예정일 7일 전까지 열람청구서에 열람사유서를 첨부하여 경찰관서에 제출하여야 한다. 다만, 긴급하거나 부득이한 사유가 있음을 소명하는 경우에는 그러하지 아니하다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 시행규칙 제37조의3 제2항', 'full_text': '② 제1항에 따라 정보의 제공을 요청받은 경찰서장은 법 제53조의4에 따라 어린이 교육시설을 감독하는 주무기관의 장이 요청한 정보를 해당 주무기관의 장에게 제공할 수 있다. [본조신설 2014.12.31.]', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제74조 제2항', 'full_text': '② 지방경찰청장은 교통안전교육을 하기 위하여 다음 각 호의 어느 하나에 해당하는 기관이나 시설이 대통령령으로 정하는 시설ㆍ설비 및 강사 등의 요건을 갖추어 신청하는 경우에는 해당 기관이나 시설을 교통안전교육을 하는 기관(이하 \"\"교통안전교육기관\"\"이라 한다)으로 지정할 수 있다. 1. 제99조에 따른 자동차운전학원 2. 제120조 및 제121조에 따른 도로교통공단과 그 지부(支部)ㆍ지소 및 교육기관 3. 「평생교육법」 제30조제2항에 따른 평생교육과정이 개설된 대학 부설 평생교육시설 4. 제주특별자치도 또는 시ㆍ군ㆍ자치구에서 운영하는 교육시설', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '교통사고조사규칙 제15조 제4항', 'full_text': '④ 사진촬영을 할 때에는 목적물의 방향과 남은 흔적 등에 주의하고, 반드시 그 크기를 파악할 수 있도록 하여야 한다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "\n",
      "\n",
      "Top five entries by rouge_1:\n",
      "\n",
      "{'label': '도로교통법 시행규칙 제19조 제3항', 'full_text': '③ 경찰청장 또는 지방경찰청장이 법 제17조제2항에 따라 구역 또는 구간을 지정하여 자동차등의 속도를 제한하려는 경우에는 「도로의 구조ㆍ시설기준에 관한 규칙」 제8조에 따른 설계속도, 실제 주행속도, 교통사고 발생 위험성, 도로주변 여건 등을 고려하여야 한다.<신설 2010.7.9.>', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '자동차손해배상 보장법 시행령 제12조의3 제2항', 'full_text': '② 보험회사등이 법 제14조제4항 전단에 따라 교통사고 관련 조사기록의 열람을 청구하는 경우에는 열람예정일 7일 전까지 열람청구서에 열람사유서를 첨부하여 경찰관서에 제출하여야 한다. 다만, 긴급하거나 부득이한 사유가 있음을 소명하는 경우에는 그러하지 아니하다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 시행규칙 제37조의3 제2항', 'full_text': '② 제1항에 따라 정보의 제공을 요청받은 경찰서장은 법 제53조의4에 따라 어린이 교육시설을 감독하는 주무기관의 장이 요청한 정보를 해당 주무기관의 장에게 제공할 수 있다. [본조신설 2014.12.31.]', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제74조 제2항', 'full_text': '② 지방경찰청장은 교통안전교육을 하기 위하여 다음 각 호의 어느 하나에 해당하는 기관이나 시설이 대통령령으로 정하는 시설ㆍ설비 및 강사 등의 요건을 갖추어 신청하는 경우에는 해당 기관이나 시설을 교통안전교육을 하는 기관(이하 \"\"교통안전교육기관\"\"이라 한다)으로 지정할 수 있다. 1. 제99조에 따른 자동차운전학원 2. 제120조 및 제121조에 따른 도로교통공단과 그 지부(支部)ㆍ지소 및 교육기관 3. 「평생교육법」 제30조제2항에 따른 평생교육과정이 개설된 대학 부설 평생교육시설 4. 제주특별자치도 또는 시ㆍ군ㆍ자치구에서 운영하는 교육시설', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '교통사고조사규칙 제15조 제4항', 'full_text': '④ 사진촬영을 할 때에는 목적물의 방향과 남은 흔적 등에 주의하고, 반드시 그 크기를 파악할 수 있도록 하여야 한다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "\n",
      "\n",
      "Top five entries by rouge_l:\n",
      "\n",
      "{'label': '도로교통법 시행규칙 제19조 제3항', 'full_text': '③ 경찰청장 또는 지방경찰청장이 법 제17조제2항에 따라 구역 또는 구간을 지정하여 자동차등의 속도를 제한하려는 경우에는 「도로의 구조ㆍ시설기준에 관한 규칙」 제8조에 따른 설계속도, 실제 주행속도, 교통사고 발생 위험성, 도로주변 여건 등을 고려하여야 한다.<신설 2010.7.9.>', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '자동차손해배상 보장법 시행령 제12조의3 제2항', 'full_text': '② 보험회사등이 법 제14조제4항 전단에 따라 교통사고 관련 조사기록의 열람을 청구하는 경우에는 열람예정일 7일 전까지 열람청구서에 열람사유서를 첨부하여 경찰관서에 제출하여야 한다. 다만, 긴급하거나 부득이한 사유가 있음을 소명하는 경우에는 그러하지 아니하다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 시행규칙 제37조의3 제2항', 'full_text': '② 제1항에 따라 정보의 제공을 요청받은 경찰서장은 법 제53조의4에 따라 어린이 교육시설을 감독하는 주무기관의 장이 요청한 정보를 해당 주무기관의 장에게 제공할 수 있다. [본조신설 2014.12.31.]', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제74조 제2항', 'full_text': '② 지방경찰청장은 교통안전교육을 하기 위하여 다음 각 호의 어느 하나에 해당하는 기관이나 시설이 대통령령으로 정하는 시설ㆍ설비 및 강사 등의 요건을 갖추어 신청하는 경우에는 해당 기관이나 시설을 교통안전교육을 하는 기관(이하 \"\"교통안전교육기관\"\"이라 한다)으로 지정할 수 있다. 1. 제99조에 따른 자동차운전학원 2. 제120조 및 제121조에 따른 도로교통공단과 그 지부(支部)ㆍ지소 및 교육기관 3. 「평생교육법」 제30조제2항에 따른 평생교육과정이 개설된 대학 부설 평생교육시설 4. 제주특별자치도 또는 시ㆍ군ㆍ자치구에서 운영하는 교육시설', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "{'label': '교통사고조사규칙 제15조 제4항', 'full_text': '④ 사진촬영을 할 때에는 목적물의 방향과 남은 흔적 등에 주의하고, 반드시 그 크기를 파악할 수 있도록 하여야 한다.', 'avg_bleu_1': 0.0, 'avg_bleu_4': 0.0, 'avg_rouge_1': 0.0, 'avg_rouge_l': 0.0, 'avg_gpt_score': 0.0}\n",
      "\n",
      "\n",
      "Top five entries by gpt_score:\n",
      "\n",
      "{'label': '도로교통법 시행령 제79조 제1항', 'full_text': '제79조 (위탁업무의 수행)  ① 법 제123조제13호에 따라 공단이 국가 또는 지방자치단체로부터 위탁받아 수행할 수 있는 도로교통안전에 관한 업무는 다음 각 호와 같다. 1. 도로교통안전에 관한 정책개발을 위한 연구 2. 도로교통사고에 관한 감정 업무 3. 교통안전시설의 설치ㆍ관리 업무 4. 운전자 교육 등 도로교통안전에 종사하는 사람에 대한 교육훈련 5. 원활한 도로교통 소통을 위한 각종 정보의 수집ㆍ분석 및 제공에 관한 업무 6. 무인 교통단속용 장비의 운영ㆍ관리에 관한 업무 7. 그 밖에 도로교통안전에 관한 업무', 'avg_bleu_1': 0.004166666666666667, 'avg_bleu_4': 0.0007805482439043401, 'avg_rouge_1': 0.007352941176470588, 'avg_rouge_l': 0.007352941176470588, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제114조', 'full_text': '제114조 (청문) 지방경찰청장은 제113조에 따라 학원등의 등록 또는 지정을 취소하려면 청문을 하여야 한다. [전문개정 2011.6.8.] ', 'avg_bleu_1': 0.0039941902687000725, 'avg_bleu_4': 0.0007288064004627166, 'avg_rouge_1': 0.007729029324846556, 'avg_rouge_l': 0.007729029324846556, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 시행규칙 제27조 제1항', 'full_text': '제27조 (정비불량표지 등)  ①영 제24조제1항에 따른 정비불량표지는 별표 13에 의하고, 정비명령서는 별지 제14호서식에 의한다.', 'avg_bleu_1': 0.1320146030137897, 'avg_bleu_4': 0.03910363046582317, 'avg_rouge_1': 0.1661967418546366, 'avg_rouge_l': 0.1661967418546366, 'avg_gpt_score': 0.0}\n",
      "{'label': '도로교통법 제13조 제4항', 'full_text': '④ 차마의 운전자는 제3항에도 불구하고 다음 각 호의 어느 하나에 해당하는 경우에는 도로의 중앙이나 좌측 부분을 통행할 수 있다. 1. 도로가 일방통행인 경우 2. 도로의 파손, 도로공사나 그 밖의 장애 등으로 도로의 우측 부분을 통행할 수 없는 경우 3. 도로 우측 부분의 폭이 6미터가 되지 아니하는 도로에서 다른 차를 앞지르려는 경우. 다만, 다음 각 목의 어느 하나에 해당하는 경우에는 그러하지 아니하다. 가. 도로의 좌측 부분을 확인할 수 없는 경우 나. 반대 방향의 교통을 방해할 우려가 있는 경우 다. 안전표지 등으로 앞지르기를 금지하거나 제한하고 있는 경우 4. 도로 우측 부분의 폭이 차마의 통행에 충분하지 아니한 경우 5. 가파른 비탈길의 구부러진 곳에서 교통의 위험을 방지하기 위하여 지방경찰청장이 필요하다고 인정하여 구간 및 통행방법을 지정하고 있는 경우에 그 지정에 따라 통행하는 경우', 'avg_bleu_1': 0.0875, 'avg_bleu_4': 0.026801772547513432, 'avg_rouge_1': 0.10267857142857145, 'avg_rouge_l': 0.10267857142857145, 'avg_gpt_score': 0.0}\n",
      "{'label': '교통사고조사규칙 제6조 제3항', 'full_text': '③ 경찰공무원은 사상자 구호 및 현장조사가 종료한 때에는 즉시 교통통제 등의 조치를 해제하여 정상적인 교통소통이 될 수 있도록 하여야 한다.', 'avg_bleu_1': 0.03125, 'avg_bleu_4': 0.010042855236808148, 'avg_rouge_1': 0.03571428571428572, 'avg_rouge_l': 0.03571428571428572, 'avg_gpt_score': 0.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    print(f\"Top five entries by {metric}:\\n\")\n",
    "    with open(os.path.join(final_dir, f\"avg_{metric}.json\"), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(5):\n",
    "            print(data[i])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the top 10% and bottom 10% for each metric into a new JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = float(0.1)\n",
    "for metric in metrics:\n",
    "    with open(os.path.join(final_dir, f\"avg_{metric}.json\"), \"r\") as avg_file:\n",
    "        data = json.load(avg_file)\n",
    "        with open(os.path.join(final_dir, f\"top_{str(int(percent*100))}_{metric}.json\"), \"w\") as f:\n",
    "            json.dump(data[:int(len(data)*percent)], f, indent=2, ensure_ascii=False)\n",
    "        with open(os.path.join(final_dir, f\"bottom_{str(int(percent*100))}_{metric}.json\"), \"w\") as f:\n",
    "            json.dump(data[-int(len(data)*percent):], f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
